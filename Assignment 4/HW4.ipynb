{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_size: int, environment_name: str, parallel_game_unrolls: int, observation_preprocessing_function: callable, unroll_steps:int):\n",
    "        self.max_size = max_size\n",
    "        self.environment_name = environment_name\n",
    "        self.parallel_game_unrolls = parallel_game_unrolls\n",
    "        self.unroll_steps = unroll_steps\n",
    "        self.observation_preprocessing_function = observation_preprocessing_function\n",
    "        self.num_possible_actions = gym.make(environment_name).action_space.n\n",
    "        self.envs = gym.vector.make(environment_name, num_envs=self.parallel_game_unrolls)\n",
    "        self.current_states, _ = self.envs.reset()\n",
    "        self.data = []\n",
    "\n",
    "    def fill_with_samples(self, dqn_network, epsilon: float):\n",
    "        # adds new samples into the ERP\n",
    "\n",
    "        states_list = []\n",
    "        actions_list = []\n",
    "        rewards_list = []\n",
    "        terminateds_list = []\n",
    "        next_states_list = []\n",
    "\n",
    "        \n",
    "        for i in range(self.unroll_steps):\n",
    "            actions = self.sample_epsilon_greedy(dqn_network, epsilon) # (PARALLEL_GAME_UNROLLS, )\n",
    "            # take the action and get s' and r\n",
    "            next_states, rewards, terminateds, _, _ = self.envs.step(actions)\n",
    "            # store observation, action, reward, next_observation into ERP container\n",
    "            #\n",
    "            states_list.append(self.current_states)\n",
    "            actions_list.append(actions)\n",
    "            rewards_list.append(rewards)\n",
    "            terminateds_list.append(terminateds)\n",
    "            next_states_list.append(next_states)\n",
    "            self.current_states = next_states\n",
    "\n",
    "        def data_generator():\n",
    "            for states_batch, actions_batch, rewards_batch, terminateds_batch, next_states_batch in \\\n",
    "                zip(states_list, actions_list, rewards_list, terminateds_list, next_states_list):\n",
    "                for game_idx in range(self.parallel_game_unrolls):\n",
    "                    state = states_batch[game_idx,:,:,:]\n",
    "                    action = actions_batch[game_idx]\n",
    "                    reward = rewards_batch[game_idx]\n",
    "                    terminated = terminateds_batch[game_idx]\n",
    "                    next_state = next_states_batch[game_idx,:,:,:]\n",
    "                    yield(state, action, reward, next_state, terminated)\n",
    "        \n",
    "        dataset_tensor_specs = (tf.TensorSpec(shape=(210,160,3), dtype=tf.uint8), \n",
    "                                tf.TensorSpec(shape=(), dtype=tf.int32), \n",
    "                                tf.TensorSpec(shape=(), dtype=tf.float32), \n",
    "                                tf.TensorSpec(shape=(210,160,3), dtype=tf.uint8),\n",
    "                                tf.TensorSpec(shape=(), dtype=tf.bool))\n",
    "        new_samples_dataset = tf.data.Dataset.from_generator(data_generator, output_signature=dataset_tensor_specs)\n",
    "        \n",
    "        new_samples_dataset = new_samples_dataset.map(lambda state, action, reward, next_state, terminated:(self.observation_preprocessing_function(state), action, reward, self.observation_preprocessing_function(next_state), terminated))\n",
    "        new_samples_dataset = new_samples_dataset.cache().shuffle(buffer_size=self.unroll_steps * self.parallel_game_unrolls, reshuffle_each_iteration=True)\n",
    "\n",
    "        for elem in new_samples_dataset:\n",
    "            continue\n",
    "\n",
    "        self.data.append(new_samples_dataset)\n",
    "\n",
    "        datapoints_in_data = len(self.data) * self.parallel_game_unrolls * self.unroll_steps\n",
    "        if datapoints_in_data > self.max_size:\n",
    "            self.data.pop(0)\n",
    "\n",
    "\n",
    "    def create_dataset(self):\n",
    "        ERP_dataset = tf.data.Dataset.sample_from_datasets(self.data, weights=[1/float(len(self.data)) for _ in self.data], stop_on_empty_dataset = False)\n",
    "        return ERP_dataset\n",
    "\n",
    "    def sample_epsilon_greedy(self, dqn_network, epsilon):\n",
    "        observations = self.observation_preprocessing_function(self.current_states)\n",
    "        q_values = dqn_network(observations) # tensor float 32 shape(parallel_game_unrolls, num_actions)\n",
    "        greedy_actions = tf.argmax(q_values, axis=1)\n",
    "        random_actions = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=self.num_possible_actions, dtype=tf.int64)\n",
    "        epsilon_sampling = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=1, dtype=tf.float32) > epsilon\n",
    "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy()\n",
    "        return actions\n",
    "\n",
    "def observation_preprocessing_function(observation):\n",
    "    # preprocess our observation so that it has shape (84, 84) and is between -1 and 1\n",
    "    observation = tf.image.resize(observation, size=(84,84))\n",
    "    observation = tf.cast(observation, dtype=tf.float32)/128.0 - 1.0\n",
    "    return observation\n",
    "\n",
    "def create_dqn_model(num_actions: int):\n",
    "    # create intput for function tf model api\n",
    "    input_layer = tf.keras.Input(shape=(84,84,3), dtype=tf.float32)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(input_layer) # (84, 84, 3)\n",
    "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(input_layer) + x # residual connections\n",
    "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(input_layer) + x\n",
    "\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x) # (42, 42, )\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) # (42, 42, )\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) + x\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) + x\n",
    "\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x) #(21, 21, )\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) #(21, 21, )\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x \n",
    "\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x) #(10, 10, )\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x\n",
    "\n",
    "    x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(units=64, activation='relu')(x) + x\n",
    "    x = tf.keras.layers.Dense(units=num_actions, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_dqn(train_dqn_network, target_network, dataset, optimizer, gamma: float, num_training_steps: int, batch_size: int=256):\n",
    "    dataset = dataset.batch(batch_size).prefetch(4)\n",
    "    @tf.function\n",
    "    def training_step(q_target, observations, actions):\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_predictions_all_actions = train_dqn_network(observations) # shape of q_predictions is (batch_size, num_actions)\n",
    "            q_predictions = tf.gather(q_predictions_all_actions, actions, batch_dims=1)\n",
    "            loss = tf.reduce_mean(tf.square(q_predictions - q_target))\n",
    "        gradients = tape.gradient(loss, train_dqn_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, train_dqn_network.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    losses = []\n",
    "    q_values = []\n",
    "    for i, state_transition in enumerate(dataset):\n",
    "        state, action, reward, subsequent_state, terminated = state_transition\n",
    "        # calculate q_target\n",
    "        #print(subsequent_state.shape)\n",
    "        q_vals = target_network(subsequent_state)\n",
    "        q_values.append(q_vals.numpy())\n",
    "        max_q_values = tf.reduce_max(q_vals, axis=1)\n",
    "        use_subsequent_state = tf.where(terminated, tf.zeros_like(max_q_values, dtype=tf.float32), tf.ones_like(max_q_values, dtype=tf.float32))\n",
    "        q_target = reward + (gamma*max_q_values*use_subsequent_state)\n",
    "        loss = training_step(q_target, observations=state, actions=action)\n",
    "        losses.append(loss)\n",
    "        if i >= num_training_steps:\n",
    "            break\n",
    "    return np.mean(losses), np.mean(q_values)\n",
    "\n",
    "def test_q_network(test_dqn_network, environment_name: str, num_parallel_tests: int, gamma: float, preprocessing_function: callable, test_epsilon: float=0.05):\n",
    "    envs = gym.vector.make(environment_name, num_parallel_tests)\n",
    "    num_possible_actions = envs.single_action_space.n\n",
    "    states, _ = envs.reset()\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    # episodes_finished is np vector of shape (num_parallel_tests,), filled with booleans, starting with all False\n",
    "    episodes_finished = np.zeros(num_parallel_tests, dtype=bool)\n",
    "    returns = np.zeros(num_parallel_tests)\n",
    "    test_steps = 0\n",
    "    while not done:\n",
    "        states = preprocessing_function(states)\n",
    "        q_values = test_dqn_network(states)\n",
    "        greedy_actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64, shape (num_parallel_tests,)\n",
    "        random_actions = tf.random.uniform(shape=(num_parallel_tests, ), minval=0,\n",
    "                                           maxval=num_possible_actions, dtype=tf.int64)\n",
    "        epsilon_sampling = tf.random.uniform(shape=(num_parallel_tests,), minval=0,\n",
    "                                             maxval=1, dtype=tf.float32) > test_epsilon # tensor of type tf.bool, shape (num_parallel_tests,)\n",
    "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy() # tensor of type tf.int64, shape (num_parallel_tests,)\n",
    "        states, rewards, terminateds, _, _ = envs.step(actions)\n",
    "        # compute pointwise or between episodes_finished and terminateds\n",
    "        episodes_finished = np.logical_or(episodes_finished, terminateds)\n",
    "        returns += ((gamma**timestep)*rewards)*(np.logical_not(episodes_finished).astype(np.float32))\n",
    "        timestep += 1\n",
    "        # done if all episodes are finished\n",
    "        done = np.all(episodes_finished)\n",
    "        test_steps += 1\n",
    "        if test_steps % 100 == 0:\n",
    "           print(f\"test_steps: {test_steps} {np.sum(episodes_finished)/num_parallel_tests} {terminateds.shape} {episodes_finished.shape}\")\n",
    "    return np.mean(returns)\n",
    "\n",
    "def visualize_q_values(results_df, step):\n",
    "    # create three subplots\n",
    "    fig, axis = plt.subplots(1, 3)\n",
    "    # include the row idxs explicitly in the results_df\n",
    "    results_df['step'] = results_df.index\n",
    "    # plot the average return\n",
    "    sns.lineplot(x='step', y='average_return', data=results_df, ax=axis[0])\n",
    "    # plot the average loss\n",
    "    sns.lineplot(x='step', y='average_loss', data=results_df, ax=axis[1])\n",
    "    # plot the average q values\n",
    "    sns.lineplot(x='step', y='average_q_values', data=results_df, ax=axis[2])\n",
    "    # save the figure\n",
    "    # create a timestring from the timestamp\n",
    "    timestring = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # save the figure\n",
    "    plt.savefig(f\"./{timestring}_results_step{step}.png\")\n",
    "    # close the figure\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def polyak_averaging_weights(source_network, target_network, polyak_averaging_factor: float):\n",
    "    source_network_weights = source_network.get_weights()\n",
    "    target_network_weights = target_network.get_weights()\n",
    "    averaged_weights = []\n",
    "    for source_weight, target_weight in zip(source_network_weights, target_network_weights):\n",
    "        fraction_kept_weights = polyak_averaging_factor * target_weight\n",
    "        fraction_updated_weights = (1-polyak_averaging_factor) * source_weight\n",
    "        averaged_weight = fraction_kept_weights + fraction_updated_weights\n",
    "        averaged_weights.append(averaged_weight)\n",
    "    target_network.set_weights(averaged_weights)\n",
    "\n",
    "def dqn():\n",
    "    ENVIRONMENT_NAME = \"ALE/Breakout-v5\"\n",
    "    NUMBER_ACTIONS = gym.make(ENVIRONMENT_NAME).action_space.n\n",
    "    ERP_SIZE = 10000\n",
    "    PARALLEL_GAME_UNROLLS = 64\n",
    "    UNROLL_STEPS = 4\n",
    "    EPSILON = 0.2\n",
    "    GAMMA = 0.98\n",
    "    NUM_TRAINING_STEPS_PER_ITER = 4\n",
    "    NUM_TRAINING_ITERS = 50000\n",
    "    TEST_EVERY_N_STEPS = 1000\n",
    "    TEST_NUM_PARALLEL_ENVS = 128\n",
    "    PREFILL_STEPS = 50\n",
    "    POLYAK_AVERAGING_FACTOR = 0.99\n",
    "    erp = ExperienceReplayBuffer(max_size=ERP_SIZE, environment_name=ENVIRONMENT_NAME, \n",
    "                                 parallel_game_unrolls=PARALLEL_GAME_UNROLLS, unroll_steps=UNROLL_STEPS,\n",
    "                                 observation_preprocessing_function=observation_preprocessing_function)\n",
    "    \n",
    "    # This is the DQN we train\n",
    "    dqn_agent = create_dqn_model(num_actions=NUMBER_ACTIONS)\n",
    "    # This is the target network, used to calculate the q-estimation targets\n",
    "    target_network = create_dqn_model(num_actions=NUMBER_ACTIONS)\n",
    "    dqn_agent.summary()\n",
    "    # test the agent\n",
    "    dqn_agent(tf.random.uniform(shape=(1, 84, 84, 3)))\n",
    "    # copy over the weights from the dqn_agent to the target_network via polyak averaging with factor 0.0\n",
    "    polyak_averaging_weights(dqn_agent, target_network, polyak_averaging_factor=0.0)\n",
    "\n",
    "    dqn_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    return_tracker = []\n",
    "    dqn_prediction_error = []\n",
    "    average_q_values = []\n",
    "\n",
    "    # prefill the replay buffer\n",
    "    prefill_exploration_epsilon = 1.\n",
    "    for prefill_step in range(PREFILL_STEPS):\n",
    "        erp.fill_with_samples(dqn_agent, epsilon=prefill_exploration_epsilon)\n",
    "\n",
    "\n",
    "    for step in range(NUM_TRAINING_ITERS):\n",
    "        print(f'Training step: {step}')\n",
    "        # step 1: put some s, a, r, s', t transitions into the replay buffer\n",
    "        erp.fill_with_samples(dqn_agent, epsilon=EPSILON)\n",
    "        dataset = erp.create_dataset()\n",
    "        # step 2: train on some samples from the replay buffer\n",
    "        average_loss, average_q_vals = train_dqn(dqn_agent, target_network, dataset, dqn_optimizer, gamma=GAMMA, num_training_steps=NUM_TRAINING_STEPS_PER_ITER)\n",
    "        # update the target network via polyak averaging\n",
    "        polyak_averaging_weights(dqn_agent, target_network, polyak_averaging_factor=POLYAK_AVERAGING_FACTOR)\n",
    "\n",
    "        if step % TEST_EVERY_N_STEPS == 0:\n",
    "            average_return = test_q_network(dqn_agent, ENVIRONMENT_NAME, num_parallel_tests=TEST_NUM_PARALLEL_ENVS, gamma=GAMMA,\n",
    "                                            preprocessing_function=observation_preprocessing_function)\n",
    "            return_tracker.append(average_return)\n",
    "            dqn_prediction_error.append(average_loss)\n",
    "            average_q_values.append(average_q_vals)\n",
    "            # print average returns, average loss, average q values\n",
    "            print(f\"TESTING: Average return: {average_return}, Average loss: {average_loss}, Average q value-estimation: {average_q_vals}\")\n",
    "            # put all result lists into a dataframe by transforming them into a dict first\n",
    "            results_dict = {'average_return': return_tracker, 'average_loss': dqn_prediction_error, 'average_q_values': average_q_values}\n",
    "            results_df = pd.DataFrame(results_dict)\n",
    "            # visualize\n",
    "            visualize_q_values(results_df, step)\n",
    "            print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
